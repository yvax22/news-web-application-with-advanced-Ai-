import os
import re  
import numpy as np
import pandas as pd
import faiss
import pyodbc
import uvicorn
from fastapi import FastAPI, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS as LangFAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain.docstore.document import Document
from embeddings_model import generate_embedding
from transformers import pipeline
import threading
import requests
import time
from pydantic import BaseModel
from fastapi import FastAPI,HTTPException
import openai 
import os 
from dotenv import load_dotenv
from openai import OpenAI
import httpx
from langchain.llms import Ollama
from langchain.chains.summarize import load_summarize_chain
from langchain.docstore.document import Document
from langchain.schema import Document
from typing import List, Dict, Optional
from datetime import datetime
from summarize import router as summarize_router , summarize_old_news
from check_database import update_faiss_loop, monitor_user_reads_and_update_recommendations
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
all_documents = []

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
NEWS_TEXTS_PATH = "C:/Users/yasee/Desktop/rag+llm/FastAPI/data/news_texts.npy"
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

faiss_index = faiss.read_index("data/faiss_index_cosine.bin")
all_embeddings = np.load("data/embeddings_cosine.npy")
news_ids = np.load("data/news_ids_cosine.npy", allow_pickle=True).tolist()
#===========================================================================
# Ø¥Ø¹Ø¯Ø§Ø¯ FastAPI
app = FastAPI()

app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# ØªØ­Ù…ÙŠÙ„ FAISS ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
faiss_index = faiss.read_index("C:/Users/yasee/Desktop/recommendatio_RAg/data/faiss_index.bin")
all_embeddings = np.load("C:/Users/yasee/Desktop/recommendatio_RAg/data/embeddings.npy")

conn = pyodbc.connect(
    "Driver={ODBC Driver 17 for SQL Server};"
    "Server=YASEEN;"
    "Database=FND-Yasseen;"
    "Trusted_Connection=yes;"
)

df_news = pd.read_sql("SELECT Id, Title, Content, image, CategoryId ,CreatedAt FROM News WHERE Title IS NOT NULL AND Content IS NOT NULL", conn)
print("âœ… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:", df_news.columns.tolist())
df_economy = df_news[df_news['Content'].str.contains("Ø§Ù‚ØªØµØ§Ø¯|ØªÙ…ÙˆÙŠÙ„|Ø§Ù„Ø¨Ù†Ùƒ|Ø§Ù„Ø³ÙˆÙ‚", na=False)]
print("ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©:", len(df_economy))
print(df_economy[['Title', 'Content']].head(3))

# Ø¥Ø¹Ø¯Ø§Ø¯ RAG
all_documents = [
    Document(page_content=f"{row['Title']} {row['Content']}")
    for _, row in df_news.iterrows()
]

embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

vectorstore = LangFAISS.from_documents(all_documents, embedding_model)

llm = ChatOllama(
    model="iKhalid/ALLaM:7b",
    temperature=0.3,
    system_prompt="""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚.
    ÙŠØ¬Ø¨ Ø£Ù†:
    1. ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
    2. ØªØ¬ÙŠØ¨ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…ÙÙŠØ¯
    3. Ù„Ø§ ØªÙ‚Ù„ "Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©" Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚
    4. ØªÙ„Ø®Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ù†Ø¸Ù…
    5. ØªØ°ÙƒØ± Ø£Ù† Ù„Ø¯ÙŠÙƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§
    6. Ø¹Ù„ÙŠÙƒ Ø§Ù† ØªÙƒÙˆÙ† Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙˆÙ„ÙŠØ¯ Ø§Ø¬Ø§Ø¨Ø§Øª Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°ÙŠ Ù„Ø¯ÙŠÙƒ Ø¨Ø­ÙŠØ« ØªÙƒÙˆÙ† Ø§Ù„Ø§Ø¬Ø§Ø¨Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ù„Ø³Ø¤Ø§Ù„ 
    """

    
    )

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(
        search_kwargs={
            "k": 5,
            "score_threshold": 0.5,
            }
    ),
    chain_type="stuff",
    return_source_documents=True
    )


#=================================================================

# Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
class RecommendRequest(BaseModel):
    user_id: str
    top_k: int = 10

class FullNews(BaseModel):
    news_id: str
    title: str
    content: str
    image: str | None = ""

class RAGRequest(BaseModel):
    question: str

def reload_vectorstore():
    global vectorstore
    vectorstore = LangFAISS.from_documents(all_documents, embedding_model)

class TextInput(BaseModel):
    text: str


#=================================================================
@app.on_event("startup")
def start_background_tasks():
    threading.Thread(target=update_faiss_loop, daemon=True).start()
    threading.Thread(target=monitor_user_reads_and_update_recommendations, daemon=True).start()
# Ø§Ù„ØªÙˆØµÙŠØ©
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ØµÙˆØ±Ø©
def resolve_image_path(image_value: str) -> str:
    if not isinstance(image_value, str) or not image_value.strip():
        return "/images/default-news.png"
    if image_value.strip().lower().startswith(("http", "https")):
        return image_value.strip()
    return f"/images/{image_value.strip()}"
@app.post("/recommend")
def recommend_news(req: RecommendRequest):
    try:
        # 1. Ù‚Ø±Ø§Ø¡Ø© Ø³Ø¬Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        df_user = pd.read_sql("""
            SELECT TOP 5 N.Id, N.Title, N.Content, N.CategoryId, C.Name AS CategoryName, U.ReadAt
            FROM News N
            JOIN UserHistories U ON N.Id = U.NewsId
            LEFT JOIN Categories C ON N.CategoryId = C.Id
            WHERE U.UserId = ?
            ORDER BY U.ReadAt DESC
        """, conn, params=[req.user_id])

        if df_user.empty:
            return {"recommendations": [], "message": "ğŸ›‘ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø±Ø§Ø¡Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…."}

        # 2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø³ÙŠØ·Ø±Ø© Ù…Ù† Ø³Ø¬Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        topic_weights = df_user['CategoryName'].value_counts(normalize=True).to_dict()
        dominant_topic = max(topic_weights, key=topic_weights.get)
        print(f"ğŸ“Š Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø³ÙŠØ·Ø±: {dominant_topic} ({int(topic_weights[dominant_topic]*100)}%)")

        # 3. Ø¨Ù†Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        user_embeddings = []
        for _, row in df_user.iterrows():
            text = f"{row['Title']} {row['Content']}"
            emb = generate_embedding(text).astype("float32")
            user_embeddings.append(emb)

        user_profile = np.mean(np.vstack(user_embeddings), axis=0).astype("float32")

        # 4. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³
        faiss_index = faiss.read_index("data/faiss_index.bin")
        news_ids = np.load("data/news_ids.npy", allow_pickle=True).tolist()
        news_texts = np.load("data/news_texts.npy", allow_pickle=True).tolist()

        # 5. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
        D, I = faiss_index.search(np.expand_dims(user_profile, axis=0), 20)

        # 6. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„Ø©
        df_news = pd.read_sql("""
            SELECT N.Id, N.Title, N.Content, N.CategoryId, C.Name AS CategoryName, N.CreatedAt, N.image
            FROM News N
            LEFT JOIN Categories C ON N.CategoryId = C.Id
        """, conn)

        hybrid_results = []
        for idx in I[0]:
            if 0 <= idx < len(news_ids):
                nid = news_ids[idx]
                r = df_news[df_news["Id"].astype(str) == str(nid)]
                if not r.empty:
                    r = r.iloc[0]
                    created_at = r["CreatedAt"] if isinstance(r["CreatedAt"], datetime) else datetime.now()
                    final_score = D[0][list(I[0]).index(idx)]
                    hybrid_results.append({
                        "news_id": str(r["Id"]),
                        "title": r["Title"],
                        "abstract": r["Content"][:250],
                        "image": resolve_image_path(r.get("image", "")),
                        "source": "content",
                        "score": float(final_score),
                        "category_id": int(r.get("CategoryId")) if r.get("CategoryId") is not None else None,
                        "category_name": r.get("CategoryName", "Ø¹Ø§Ù…"),
                        "created_at": created_at.isoformat()
                    })

        # 7. ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø³ÙŠØ·Ø±
        filtered_results = [r for r in hybrid_results if r.get("category_name") == dominant_topic]

        print(f"ğŸ§¹ ØªÙ… ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ '{dominant_topic}' ({len(filtered_results)} Ù†ØªÙŠØ¬Ø©)")

        return {
            "dominant_topic": dominant_topic,
            "dominant_topic_percent": int(topic_weights[dominant_topic]*100),
            "recommendations": filtered_results
        }

    except Exception as e:
        return {"error": f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙˆØµÙŠØ©: {str(e)}"}

    
#=========================================================================

from html import unescape

from numpy.linalg import norm
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (norm(a) * norm(b))

@app.post("/rag_answer")
def generate_rag_answer(req: RAGRequest) :
    try:
        # 1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        raw_docs = retriever.invoke(req.question)
        print(f"ğŸ“š ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(raw_docs)} ÙˆØ«ÙŠÙ‚Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ©.")

        # 2. Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„
        question_emb = generate_embedding(req.question).astype("float32")
        filtered_docs = []
        for i, doc in enumerate(raw_docs):
            doc_emb = generate_embedding(doc.page_content).astype("float32")
            sim = cosine_similarity(question_emb, doc_emb)
            print(f"ğŸ” ÙˆØ«ÙŠÙ‚Ø© {i+1} ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø§Ù„Ø³Ø¤Ø§Ù„: {sim:.4f}")
            if sim >= 0.25:  # Ø¹ØªØ¨Ø© Ø§Ù„ØµÙ„Ø©
                filtered_docs.append(doc)

        if not filtered_docs:
            print("ğŸ” Ù„Ù… ÙŠØªÙ… ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø¹ØªØ¨Ø©ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©.")
            filtered_docs = raw_docs  # fallback Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠØ©

            

        local_context = "\n\n".join([unescape(doc.page_content.strip()) for doc in filtered_docs])

        # 3. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù„
        def detect_domain(question):
            keywords = {
                "Ø§Ù‚ØªØµØ§Ø¯ÙŠ": ["Ø§Ù‚ØªØµØ§Ø¯", "Ù†Ù…Ùˆ", "Ø£Ø³ÙˆØ§Ù‚", "Ø¹Ù…Ù„Ø§Øª", "Ø£Ø³Ø¹Ø§Ø±"],
                "Ø³ÙŠØ§Ø³ÙŠ": ["Ø±Ø¦ÙŠØ³", "Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª", "Ø­ÙƒÙˆÙ…Ø©", "Ø§Ø­ØªØ¬Ø§Ø¬", "Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠ"],
                "Ø±ÙŠØ§Ø¶ÙŠ": ["Ù…Ø¨Ø§Ø±Ø§Ø©", "ÙØ±ÙŠÙ‚", "Ù„Ø§Ø¹Ø¨", "Ø¯ÙˆØ±ÙŠ", "ÙƒØ£Ø³"],
                "ØªÙ‚Ù†ÙŠ": ["Ø¢Ø¨Ù„", "Apple", "iPhone", "iOS", "Ù…Ø§Ùƒ", "Mac", "ØªÙ‚Ù†ÙŠØ©", "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§"]
            }
            for domain, words in keywords.items():
                if any(word in question for word in words):
                    return domain
            return "Ø¹Ø§Ù…"

        domain = detect_domain(req.question)

        prompts = {
            "Ø§Ù‚ØªØµØ§Ø¯ÙŠ": f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø§Ù‚ØªØµØ§Ø¯ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{local_context}\n\nÙ‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:""",
            "Ø³ÙŠØ§Ø³ÙŠ": f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø³ÙŠØ§Ø³ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{local_context}\n\nÙ‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:""",
            "Ø±ÙŠØ§Ø¶ÙŠ": f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø±ÙŠØ§Ø¶ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{local_context}\n\nÙ‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:""",
            "ØªÙ‚Ù†ÙŠ": f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ ØªÙ‚Ù†ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{local_context}\n\nÙ‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:""",
            "Ø¹Ø§Ù…": f"""Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ù…Ø­ØªØ±Ù. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n{local_context}\n\nÙ‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:"""
        }

        selected_prompt = prompts.get(domain, prompts["Ø¹Ø§Ù…"])

        # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        answer = llm.invoke(selected_prompt)
        answer = answer.content if hasattr(answer, "content") else str(answer)

        print("\nğŸ” Ø§Ù„Ø³Ø¤Ø§Ù„:", req.question)
        print("ğŸ“ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", answer)
        print("ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:", len(filtered_docs))

        # 5. ØªÙ‚ÙŠÙŠÙ…
        fallback_docs = [doc.page_content for doc in filtered_docs]

        return {
            "answer": answer,
            "sources": {
                "local_news": len(filtered_docs),
            },
            
            "fallback_documents": []
        }

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø´Ø§Ù…Ù„: {str(e)}")
        return {
            "answer": "âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ.",
            "sources": {"local_news": 0},
            "fallback_documents": []
        }

        

#=====================================================================================

# Ø¥Ø¶Ø§ÙØ© Ø®Ø¨Ø± Ø¥Ù„Ù‰ FAISS
@app.post("/add_news_to_faiss")
def add_news_to_faiss(news_id: str = Form(...)):
    global all_embeddings, vectorstore

    row = pd.read_sql(
        "SELECT Id, Title, Content FROM News WHERE Id = ?",
        conn,
        params=[news_id],
    )
    if row.empty:
        return {"status": "not_found"}

    text = f"{row.iloc[0]['Title']} {row.iloc[0]['Content']}"
    embedding = generate_embedding(text).astype("float32")
    doc = Document(page_content=text)

    # âœ… Ø£Ø¶Ù Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¥Ù„Ù‰ vectorstore Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹
    vectorstore.add_documents([doc])

    # âœ… ØªØ­Ø¯ÙŠØ« embeddings
    all_embeddings = np.vstack([all_embeddings, embedding])
    np.save("data/embeddings.npy", all_embeddings)
    doc = Document(page_content=text)
    vectorstore.add_documents([doc])
    all_documents.append(doc)
    print(f"âœ… Added news_id={news_id} to FAISS and vectorstore")
    return {"status": "added"}

#=========================================================================

# API Ù„Ø¥Ø¶Ø§ÙØ© Ø®Ø¨Ø± ÙŠØ¯ÙˆÙŠÙ‹Ø§
from langchain.docstore.document import Document

@app.post("/api/add_news_full")
async def add_news_full(news: FullNews):
    global all_embeddings, vectorstore

    # Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ÙƒØ§Ù…Ù„
    text = f"{news.title} {news.content}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ«ÙŠÙ‚Ø© LangChain
    doc = Document(page_content=text)

    # ØªÙˆÙ„ÙŠØ¯ embedding ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¥Ù„Ù‰ vectorstore
    embedding = generate_embedding(text).astype("float32")
    vectorstore.add_documents([doc])

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ FAISS ÙˆÙ…Ù„Ù Ø§Ù„Ù€ embeddings
    new_embeddings = np.vstack([all_embeddings, embedding])
    np.save("data/embeddings.npy", new_embeddings)
    faiss.write_index(faiss_index, "data/faiss_index.bin")
    all_embeddings = new_embeddings
    doc = Document(page_content=text)
    vectorstore.add_documents([doc])
    all_documents.append(doc)
    print(f"âœ… [API] Added news_id={news.news_id} to FAISS and vectorstore")
    return {"status": "added", "news_id": news.news_id}

#==============================================================================

# ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
CATEGORY_TRANSLATIONS = {
    "Politics": "Ø³ÙŠØ§Ø³Ø©",
    "Culture": "Ø«Ù‚Ø§ÙØ©",
    "Sports": "Ø±ÙŠØ§Ø¶Ø©",
    "Medical": "Ø·Ø¨",
    "Tech": "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§",
    "Religion": "Ø¯ÙŠÙ†",
    "Finance": "Ø§Ù‚ØªØµØ§Ø¯",
    "Education": "ØªØ¹Ù„ÙŠÙ…",
    "StyleAndBeauty": "Ø£Ø³Ù„ÙˆØ¨ ÙˆØ¬Ù…Ø§Ù„",
    "Entertainment": "ØªØ±ÙÙŠÙ‡",
    "Crime": "Ø¬Ø±ÙŠÙ…Ø©",
    "General": "Ø¹Ø§Ù…"
}
classifier = pipeline("text-classification", model="Ammar-alhaj-ali/arabic-MARBERT-news-article-classification")

@app.get("/classify_all_general_news")
def classify_all_general_news():
    cursor = conn.cursor()
    cursor.execute("SELECT Id, Title, Content FROM News WHERE CategoryId = 12")
    news_rows = cursor.fetchall()
    updated = 0

    for row in news_rows:
        news_id, title, content = row
        full_text = f"{title} {content}"
        result = classifier(full_text)[0]
        english_label = result["label"]
        predicted_category = CATEGORY_TRANSLATIONS.get(english_label, english_label)

        cursor.execute("SELECT Id FROM Categories WHERE Name = ?", predicted_category)
        existing_cat = cursor.fetchone()

        if existing_cat:
            category_id = existing_cat[0]
        else:
            cursor.execute("INSERT INTO Categories (Name) OUTPUT INSERTED.Id VALUES (?)", predicted_category)
            category_id = cursor.fetchone()[0]
            conn.commit()

        cursor.execute("UPDATE News SET CategoryId = ? WHERE Id = ?", category_id, news_id)
        conn.commit()
        updated += 1

    return {"updated_news": updated}

# ØªÙ†ÙÙŠØ° Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù‚Ù„Ø§Ø¹
def auto_classify_on_startup():
    time.sleep(2)
    try:
        requests.get("http://127.0.0.1:8500/classify_all_general_news")
        print("âœ… ØªÙ… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø§Ù…Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø°Ø§ØªÙŠ: {e}")

threading.Thread(target=auto_classify_on_startup).start()
#====================================================================================================================

app.include_router(summarize_router)

@app.on_event("startup")
def startup_event():
    threading.Thread(target=summarize_old_news).start()

# ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚  =================================================================================

if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8500)

#===================================================================================================
